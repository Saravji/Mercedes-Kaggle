{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with clustering the data and fitting XGBoost model to each cluster\n",
    "This notebook is part of my Mercedes-Benz Kaggle efforts. Competion @: https://www.kaggle.com/c/mercedes-benz-greener-manufacturing Use of data subject to conditions mentioned in above link.\n",
    "## With additional data generation:\n",
    "+ additional data will be using mean with extremes removed (not min and max as in part 1)<br>\n",
    "+ additional data will be used for training the models, while supplied training data will be used for evaluatig (testing) of the models.<br>\n",
    "\n",
    "## With clustering of the data:\n",
    "+ use Birch Clustering algorithm, 3, 4, 5, 25 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import string\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "#from sklearn.gaussian_process.kernels import Matern\n",
    "from sklearn.cluster import Birch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/train.csv', index_col = 'ID').fillna(0)\n",
    "df_train = df_train[df_train['y'] < 200] # drop one outlier data\n",
    "df_test = pd.read_csv('../data/test.csv', index_col = 'ID').fillna(0)\n",
    "li_del_columns = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construct additional features and change letter categorical to binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_dummies(c, df1, df2):\n",
    "    l = len(df1)\n",
    "    df = pd.concat([df1, df2], axis = 0)\n",
    "    df_temp = pd.get_dummies(df[c], prefix=c)\n",
    "    df = pd.concat([df_temp, df], axis = 1)\n",
    "    df = df.drop(c, axis = 1)\n",
    "    df1 = df[ : l].copy()\n",
    "    df2 = df[l : ].copy()\n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "li_columnNames = [e for e in list(df_train.columns.values) if e not in {'y', 'X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X8'}]\n",
    "li_categorical = ['X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X8']\n",
    "list_value = list(string.lowercase) + list('a' + x for x in list(string.lowercase)) + ['ba', 'bb', 'bc']\n",
    "\n",
    "df_train_eng = df_train.copy()\n",
    "df_test_eng = df_test.copy()\n",
    "df_train_eng['qty_options'] = df_train_eng[li_columnNames].sum(axis=1)\n",
    "df_test_eng['qty_options'] = df_test_eng[li_columnNames].sum(axis=1)\n",
    "\n",
    "for c in li_categorical:\n",
    "    df_train_eng, df_test_eng = my_dummies(c, df_train_eng, df_test_eng)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. some data clensing / Identifying data for possible clensing\n",
    "### Identify duplicate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Number of columns before cleaning: 581\n",
      " Column X2_a is identical to X32. Removing X32\n",
      " Column X2_ac is identical to X125. Removing X125\n",
      " Column X2_ac is identical to X227. Removing X227\n",
      " Column X2_ad is identical to X107. Removing X107\n",
      " Column X2_ag is identical to X30. Removing X30\n",
      " Column X2_ai is identical to X215. Removing X215\n",
      " Column X2_am is identical to X260. Removing X260\n",
      " Column X2_ap is identical to X16. Removing X16\n",
      " Column X2_aq is identical to X211. Removing X211\n",
      " Column X2_as is identical to X251. Removing X251\n",
      " Column X2_at is identical to X184. Removing X184\n",
      " Column X2_at is identical to X262. Removing X262\n",
      " Column X2_au is identical to X59. Removing X59\n",
      " Column X2_aw is identical to X213. Removing X213\n",
      " Column X2_aw is identical to X67. Removing X67\n",
      " Column X2_b is identical to X26. Removing X26\n",
      " Column X2_d is identical to X97. Removing X97\n",
      " Column X2_f is identical to X23. Removing X23\n",
      " Column X2_g is identical to X112. Removing X112\n",
      " Column X2_g is identical to X199. Removing X199\n",
      " Column X2_h is identical to X86. Removing X86\n",
      " Column X2_k is identical to X172. Removing X172\n",
      " Column X2_k is identical to X216. Removing X216\n",
      " Column X2_k is identical to X62. Removing X62\n",
      " Column X2_n is identical to X28. Removing X28\n",
      " Column X2_o is identical to X257. Removing X257\n",
      " Column X2_p is identical to X92. Removing X92\n",
      " Column X2_q is identical to X83. Removing X83\n",
      " Column X2_s is identical to X113. Removing X113\n",
      " Column X2_s is identical to X134. Removing X134\n",
      " Column X2_s is identical to X147. Removing X147\n",
      " Column X2_s is identical to X222. Removing X222\n",
      " Column X2_s is identical to X48. Removing X48\n",
      " Column X2_t is identical to X102. Removing X102\n",
      " Column X2_t is identical to X214. Removing X214\n",
      " Column X2_t is identical to X239. Removing X239\n",
      " Column X2_t is identical to X53. Removing X53\n",
      " Column X2_z is identical to X36. Removing X36\n",
      " Column X0_bc is identical to X277. Removing X277\n",
      " Column X102 is identical to X214. Removing X214\n",
      " Column X102 is identical to X239. Removing X239\n",
      " Column X102 is identical to X53. Removing X53\n",
      " Column X112 is identical to X199. Removing X199\n",
      " Column X113 is identical to X134. Removing X134\n",
      " Column X113 is identical to X147. Removing X147\n",
      " Column X113 is identical to X222. Removing X222\n",
      " Column X113 is identical to X48. Removing X48\n",
      " Column X118 is identical to X119. Removing X119\n",
      " Column X125 is identical to X227. Removing X227\n",
      " Column X134 is identical to X147. Removing X147\n",
      " Column X134 is identical to X222. Removing X222\n",
      " Column X134 is identical to X48. Removing X48\n",
      " Column X138 is identical to X146. Removing X146\n",
      " Column X147 is identical to X222. Removing X222\n",
      " Column X147 is identical to X48. Removing X48\n",
      " Column X152 is identical to X226. Removing X226\n",
      " Column X152 is identical to X326. Removing X326\n",
      " Column X155 is identical to X360. Removing X360\n",
      " Column X17 is identical to X382. Removing X382\n",
      " Column X172 is identical to X216. Removing X216\n",
      " Column X172 is identical to X62. Removing X62\n",
      " Column X184 is identical to X262. Removing X262\n",
      " Column X213 is identical to X67. Removing X67\n",
      " Column X214 is identical to X239. Removing X239\n",
      " Column X214 is identical to X53. Removing X53\n",
      " Column X216 is identical to X62. Removing X62\n",
      " Column X222 is identical to X48. Removing X48\n",
      " Column X226 is identical to X326. Removing X326\n",
      " Column X230 is identical to X254. Removing X254\n",
      " Column X232 is identical to X279. Removing X279\n",
      " Column X239 is identical to X53. Removing X53\n",
      " Column X240 is identical to X364. Removing X364\n",
      " Column X244 is identical to X71. Removing X71\n",
      " Column X244 is identical to X84. Removing X84\n",
      " Column X253 is identical to X385. Removing X385\n",
      " Column X253 is identical to X60. Removing X60\n",
      " Column X290 is identical to X293. Removing X293\n",
      " Column X290 is identical to X330. Removing X330\n",
      " Column X293 is identical to X330. Removing X330\n",
      " Column X295 is identical to X296. Removing X296\n",
      " Column X298 is identical to X299. Removing X299\n",
      " Column X302 is identical to X44. Removing X44\n",
      " Column X31 is identical to X35. Removing X35\n",
      " Column X31 is identical to X37. Removing X37\n",
      " Column X324 is identical to X58. Removing X58\n",
      " Column X33 is identical to X39. Removing X39\n",
      " Column X35 is identical to X37. Removing X37\n",
      " Column X385 is identical to X60. Removing X60\n",
      " Column X54 is identical to X76. Removing X76\n",
      " Column X71 is identical to X84. Removing X84\n",
      "\n",
      " Number of identified duplicates marked: 62\n"
     ]
    }
   ],
   "source": [
    "all_data = pd.concat((df_train_eng, df_test_eng))\n",
    "\n",
    "# remove duplicated columns\n",
    "c = all_data.columns\n",
    "print('\\n Number of columns before cleaning: %d' % len(c))\n",
    "li_duplicates = []\n",
    "for i in range(len(c)-1):\n",
    "    v = all_data[c[i]].values\n",
    "    for j in range(i+1,len(c)):\n",
    "        if np.array_equal(v,all_data[c[j]].values):\n",
    "            li_duplicates.append(c[j])\n",
    "            print(' Column %s is identical to %s. Removing %s' % (str(c[i]), str(c[j]), str(c[j])))\n",
    "li_duplicates = list(set(li_duplicates))\n",
    "print '\\n Number of identified duplicates marked: %s' % len(li_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping duplicate columns\n",
    "(yes there are columns duplicate and complimentary, so need to do it twice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number columns before cleaning:  581 and after cleaning:  519\n"
     ]
    }
   ],
   "source": [
    "print 'Number columns before cleaning: ', df_train_eng.shape[1],\n",
    "l = len(df_train_eng)\n",
    "df = pd.concat([df_train_eng, df_test_eng], axis = 0)\n",
    "for c in li_duplicates:\n",
    "    df.drop(c, axis=1, inplace = True)\n",
    "\n",
    "df_train_eng = df[ : l].copy()\n",
    "df_test_eng = df[l : ].copy()\n",
    "print 'and after cleaning: ', df_train_eng.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify complementary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Number of columns before cleaning: 518\n",
      " Column X128 is complimentary to X130. Removing X130\n",
      " Column X156 is complimentary to X157. Removing X157\n",
      " Column X204 is complimentary to X205. Removing X205\n",
      " Column X232 is complimentary to X263. Removing X263\n",
      "\n",
      " Number of identified complements marked: 4\n"
     ]
    }
   ],
   "source": [
    "list_index = [e for e in list(df_train_eng.columns.values) if e not in {'y_actual', 'y', 'X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X8'}]\n",
    "all_data = pd.concat((df_train_eng[list_index], df_test_eng[list_index]))\n",
    "value_compliment = {0: 1, 1: 0}\n",
    "\n",
    "# remove complimentary columns\n",
    "c = all_data.columns\n",
    "print('\\n Number of columns before cleaning: %d' % len(c))\n",
    "li_compliments = []\n",
    "for i in range(len(c)-1):\n",
    "    v = all_data[c[i]].replace(value_compliment).values\n",
    "    for j in range(i+1,len(c)):\n",
    "        if np.array_equal(v,all_data[c[j]].values):\n",
    "            li_compliments.append(c[j])\n",
    "            print(' Column %s is complimentary to %s. Removing %s' % (str(c[i]), str(c[j]), str(c[j])))\n",
    "li_compliments = list(set(li_compliments))\n",
    "print '\\n Number of identified complements marked: %s' % len(li_compliments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Droping complementary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number columns before cleaning:  519 and after cleaning:  515\n"
     ]
    }
   ],
   "source": [
    "print 'Number columns before cleaning: ', df_train_eng.shape[1],\n",
    "l = len(df_train_eng)\n",
    "df = pd.concat([df_train_eng, df_test_eng], axis = 0)\n",
    "for c in li_compliments:\n",
    "    df.drop(c, axis=1, inplace = True)\n",
    "\n",
    "df_train_eng = df[ : l].copy()\n",
    "df_test_eng = df[l : ].copy()\n",
    "print 'and after cleaning: ', df_train_eng.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's see if we have same datarows (other then y and y_actual) in train data\n",
    "<font color=red>SKIP TO \"GENERATING MORE DATA\" IF LOADING PRE-PROCESSED DATASET. THIS STEP TAKES TIME!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>mean</i> is calcualted out of all identical datarows.<br>\n",
    "<i>mean minus extremes</i> is calculated if four or more datasets are identical by removing the lowest and highest value. <br>\n",
    "<i>mean narrowing</i> is calculated if four or more datasets are identical by removing values that are more than 5% away from mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "li_columns = [e for e in list(df_train_eng.columns.values) if e not in {'y_actual', 'y'}]\n",
    "df = df_train_eng[li_columns]\n",
    "li_seen_duplicates = []\n",
    "li_duplicates = []\n",
    "print 'start finding...'\n",
    "#for i in range(235):\n",
    "for i in range(len(df)):\n",
    "    if i not in li_seen_duplicates:\n",
    "        set_pairs = {}\n",
    "        v = df.iloc[i, :].values\n",
    "        for j in range(i+1,len(df)):\n",
    "            if np.array_equal(v,df.iloc[j, :].values):\n",
    "                if i not in li_seen_duplicates: li_seen_duplicates.append(i)\n",
    "                li_seen_duplicates.append(j)\n",
    "                if i not in set_pairs: set_pairs[i] = df_train_eng.ix[df_train_eng.index.tolist()[i],'y']\n",
    "                set_pairs[j] = df_train_eng.ix[df_train_eng.index.tolist()[j],'y']\n",
    "        if set_pairs: li_duplicates.append(set_pairs)\n",
    "    if ((i > 0) & (i % 100 == 0)): print '.',\n",
    "for i in range(len(li_duplicates)):\n",
    "    values = li_duplicates[i].values()\n",
    "    val_narrow = []\n",
    "    mean = sum(values) / len(values)\n",
    "    deviate = 0.08\n",
    "    print '\\n Row IDs ', list(li_duplicates[i].keys()), ' mean: ', mean, ' with values: ', values,\n",
    "    if len(values) >= 4:\n",
    "        print ', mean minus extremes: ', (sum(values) - min(values) - max(values)) / (len(values) - 2),\n",
    "    if len(values) >= 4:\n",
    "        for v in values:\n",
    "            if mean*(1-deviate) <= v <= mean*(1+deviate):\n",
    "                val_narrow.append(v)\n",
    "        print ', mean narrowing: ', sum(val_narrow) / len(val_narrow),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's make a copy of the trainingdata and set the values for all identical row sets to the mean or mean narrow if more than 3 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_cp = df_train_eng.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(li_duplicates)):\n",
    "    values = li_duplicates[i].values()\n",
    "    mean = sum(values) / len(values)\n",
    "    if len(values) > 3:\n",
    "        for v in values:\n",
    "            if mean*(1-deviate) <= v <= mean*(1+deviate):\n",
    "                val_narrow.append(v)\n",
    "        mean = sum(val_narrow) / len(val_narrow)\n",
    "    for j in list(li_duplicates[i].keys()):\n",
    "        df_train_cp.set_value(j ,'y', mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out the dataframe as is to load in this state if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_cp.to_csv(path_or_buf='../data/train_cp.csv', index_label='ID', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_eng.to_csv(path_or_buf='../data/train_eng.21.csv', index_label='ID', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test_eng.to_csv(path_or_buf='../data/test_eng.21.csv', index_label='ID', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating some more training data\n",
    "the amount of data considering the binary features is not much, hence let's produce some more.\n",
    "I figure I'll be adding 10 rows for each pre-existing row: I'll be adding / subtracting a small percentage (probably 1% times number of rows away from start) from the goal y value. The average of the rowset stays the same; the variance is smaller to what I have seen in the dataset earlier (see checking for identical rows and the y values). This should afford me sufficient data to get good model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_cp = pd.read_csv('../data/train_cp.csv', index_col = 'ID').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train_eng = pd.read_csv('../data/train_eng.19.csv', index_col='ID').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test_eng = pd.read_csv('../data/test_eng.21.csv', index_col='ID').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_add = pd.DataFrame.from_items([(name, pd.Series(data=None, dtype=series.dtype)) for name, series in df_train_cp.iteritems()])\n",
    "df_add = pd.DataFrame(data = None, columns=df_train_cp.columns)\n",
    "y_index = df_train_cp.columns.get_loc('y')\n",
    "add_vals = 10\n",
    "add_factor = 0.01\n",
    "add_range = range(-(add_vals/2), 0) + range(1, (add_vals/2) + 1)\n",
    "for i in range(len(df_train_cp)):\n",
    "    values = list(df_train_cp.iloc[i,:])\n",
    "    li_add = []\n",
    "    for j in add_range:\n",
    "        li_add.append(values[:])\n",
    "        li_add[len(li_add)-1][y_index] = values[y_index]*(1 + j*add_factor)\n",
    "    df_add = df_add.append(pd.DataFrame(li_add, columns=df_train_cp.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_cp = pd.concat([df_train_cp, df_add], axis = 0)\n",
    "df_train_cp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_cp['y_actual'] = df_train_cp['y'] * df_train_cp['qty_options'] # Remember that one from further up? Now you see why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train_eng['y_actual'] = df_train_eng['y'] * df_train_eng['qty_options']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_cp.to_csv(path_or_buf='../data/train_cp_XL.csv', index_label='ID', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_cp = pd.read_csv('../data/train_cp_XL.csv', index_col = 'ID').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X3_a  mean y: 102.54389213  y_actual: 6252.06563224 sigma: 12.2050103031  Datasets: 4840\n",
      "X3_b  mean y: 100.278157895  y_actual: 5606.79377193 sigma: 10.7172358958  Datasets: 627\n",
      "X3_c  mean y: 101.991260849  y_actual: 6119.74591832 sigma: 12.7935021007  Datasets: 21362\n",
      "X3_d  mean y: 104.747732493  y_actual: 5909.57985247 sigma: 10.2268751422  Datasets: 3190\n",
      "X3_e  mean y: 100.079326692  y_actual: 5756.25331581 sigma: 11.0200814603  Datasets: 1793\n",
      "X3_f  mean y: 96.7962847608  y_actual: 5230.18854792 sigma: 13.3507639374  Datasets: 11825\n",
      "X3_g  mean y: 100.807721005  y_actual: 5902.35643765 sigma: 10.6391764704  Datasets: 2651\n"
     ]
    }
   ],
   "source": [
    "for i in ['X3_a', 'X3_b', 'X3_c', 'X3_d', 'X3_e', 'X3_f', 'X3_g']:\n",
    "    print i, ' mean y:',\n",
    "    print df_train_cp[(df_train_cp[i] == 1)]['y'].mean(), ' y_actual:',\n",
    "    print df_train_cp[(df_train_cp[i] == 1)]['y_actual'].mean(), 'sigma:',\n",
    "    print df_train_cp[(df_train_cp[i] == 1)]['y'].std(), ' Datasets:',\n",
    "    print df_train_cp[(df_train_cp[i] == 1)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X3_a  mean y: 102.507477273  y_actual: 6251.17375 sigma: 11.7209104558  Datasets: 440\n",
      "X3_b  mean y: 100.239649123  y_actual: 5609.35368421 sigma: 10.2040648352  Datasets: 57\n",
      "X3_c  mean y: 101.959268795  y_actual: 6117.26341401 sigma: 12.4146420334  Datasets: 1942\n",
      "X3_d  mean y: 105.114413793  y_actual: 5930.47831034 sigma: 9.87888653322  Datasets: 290\n",
      "X3_e  mean y: 100.033190184  y_actual: 5751.59521472 sigma: 10.4960935535  Datasets: 163\n",
      "X3_f  mean y: 96.4075255814  y_actual: 5209.7048186 sigma: 13.0698420549  Datasets: 1075\n",
      "X3_g  mean y: 100.428672199  y_actual: 5882.50087137 sigma: 10.1089545237  Datasets: 241\n"
     ]
    }
   ],
   "source": [
    "for i in ['X3_a', 'X3_b', 'X3_c', 'X3_d', 'X3_e', 'X3_f', 'X3_g']:\n",
    "    print i, ' mean y:',\n",
    "    print df_train_eng[(df_train_eng[i] == 1)]['y'].mean(), ' y_actual:',\n",
    "    print df_train_eng[(df_train_eng[i] == 1)]['y_actual'].mean(), 'sigma:',\n",
    "    print df_train_eng[(df_train_eng[i] == 1)]['y'].std(), ' Datasets:',\n",
    "    print df_train_eng[(df_train_eng[i] == 1)].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use df_train_cp as training data for the clustering and df_train_eng to test the quality of the clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters in train: 3  with no noise\n",
      "Silhouette Coefficient: 0.249\n",
      "Number of clusters in test: 3  with no noise\n",
      "Silhouette Coefficient: 0.242\n",
      "Number of clusters in train: 4  with no noise\n",
      "Silhouette Coefficient: 0.207\n",
      "Number of clusters in test: 4  with no noise\n",
      "Silhouette Coefficient: 0.202\n",
      "Number of clusters in train: 5  with no noise\n",
      "Silhouette Coefficient: 0.151\n",
      "Number of clusters in test: 5  with no noise\n",
      "Silhouette Coefficient: 0.146\n",
      "Number of clusters in train: 24  with no noise\n",
      "Silhouette Coefficient: 0.132\n",
      "Number of clusters in test: 24  with no noise\n",
      "Silhouette Coefficient: 0.123\n"
     ]
    }
   ],
   "source": [
    "X = df_train_cp.copy()\n",
    "X.drop('y', axis = 1, inplace = True)\n",
    "X.drop('y_actual', axis = 1, inplace = True)\n",
    "train = df_train_eng.copy()\n",
    "train.drop('y', axis = 1, inplace = True)\n",
    "train.drop('y_actual', axis = 1, inplace = True)\n",
    "test = df_test_eng.copy()\n",
    "test.drop('y', axis = 1, inplace = True)\n",
    "#X = StandardScaler().fit_transform(X)\n",
    "li_model = []\n",
    "li_cp_clusters = []\n",
    "li_train_clusters = []\n",
    "li_test_clusters = []\n",
    "\n",
    "for i in [3, 4, 5, 25]:\n",
    "    birch_model = Birch(threshold = 0.5, branching_factor = 20, n_clusters = i).fit(X)\n",
    "    labels = birch_model.labels_\n",
    "    li_cp_clusters.append(labels)\n",
    "    centroids = birch_model.subcluster_centers_\n",
    "    n_clusters = np.unique(labels).size\n",
    "    #X[str(i)] = labels\n",
    "    li_model.append(birch_model)\n",
    "    li_train_clusters.append(birch_model.predict(train))\n",
    "    li_test_clusters.append(birch_model.predict(test))\n",
    "    \n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    lab = li_train_clusters[len(li_train_clusters)-1]\n",
    "    n_clusters_ = len(set(lab)) - (1 if -1 in lab else 0)\n",
    "    print('Number of clusters in train: %d' % n_clusters_), ' with',\n",
    "    if -1 not in lab:\n",
    "        print 'no',\n",
    "    print 'noise'\n",
    "    print(\"Silhouette Coefficient: %0.3f\"\n",
    "          % metrics.silhouette_score(train, lab))\n",
    "    \n",
    "    lab = li_test_clusters[len(li_test_clusters)-1]\n",
    "    n_clusters_ = len(set(lab)) - (1 if -1 in lab else 0)\n",
    "    print('Number of clusters in test: %d' % n_clusters_), ' with',\n",
    "    if -1 not in lab:\n",
    "        print 'no',\n",
    "    print 'noise'\n",
    "    print(\"Silhouette Coefficient: %0.3f\"\n",
    "          % metrics.silhouette_score(test, lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1, 1, 0, ..., 0, 2, 1]), array([0, 0, 3, ..., 1, 2, 0]), array([4, 4, 3, ..., 0, 2, 4]), array([ 2,  9, 15, ...,  3, 20, 23])]\n"
     ]
    }
   ],
   "source": [
    "#for i in range(len(li_train_clusters)):\n",
    "for i in range(2):\n",
    "    for j in range(len(set(li_train_clusters[i]))):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X8_a</th>\n",
       "      <th>X8_b</th>\n",
       "      <th>X8_c</th>\n",
       "      <th>X8_d</th>\n",
       "      <th>X8_e</th>\n",
       "      <th>X8_f</th>\n",
       "      <th>X8_g</th>\n",
       "      <th>X8_h</th>\n",
       "      <th>X8_i</th>\n",
       "      <th>X8_j</th>\n",
       "      <th>...</th>\n",
       "      <th>X90</th>\n",
       "      <th>X91</th>\n",
       "      <th>X93</th>\n",
       "      <th>X94</th>\n",
       "      <th>X95</th>\n",
       "      <th>X96</th>\n",
       "      <th>X98</th>\n",
       "      <th>X99</th>\n",
       "      <th>qty_options</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 515 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    X8_a  X8_b  X8_c  X8_d  X8_e  X8_f  X8_g  X8_h  X8_i  X8_j ...   X90  X91  \\\n",
       "ID                                                             ...              \n",
       "1      0     0     0     0     0     0     0     0     0     0 ...     0    0   \n",
       "2      0     0     0     0     0     0     0     0     0     0 ...     0    0   \n",
       "3      0     0     0     0     0     0     0     0     0     1 ...     0    0   \n",
       "4      0     0     0     0     0     0     0     0     0     0 ...     0    0   \n",
       "5      0     0     0     0     0     0     0     0     0     0 ...     0    0   \n",
       "\n",
       "    X93  X94  X95  X96  X98  X99  qty_options    y  \n",
       "ID                                                  \n",
       "1     0    0    0    1    1    0           54  0.0  \n",
       "2     0    0    0    0    1    0           67  0.0  \n",
       "3     0    0    0    1    1    0           57  0.0  \n",
       "4     0    0    0    1    1    0           58  0.0  \n",
       "5     0    0    0    1    1    0           65  0.0  \n",
       "\n",
       "[5 rows x 515 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_eng.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
